{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d66010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8ffd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(50000, 784), y=(50000,)\n",
      "Valid: X=(10000, 784), y=(10000,)\n",
      "Test: X=(10000, 784), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "def load_fashion_mnist():\n",
    "    classes = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',\n",
    "               5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "    (trainX, trainy), (test_X, test_Y) = fashion_mnist.load_data()\n",
    "\n",
    "    train_X = trainX[10000:, :, :]\n",
    "    train_Y = trainy[10000:]\n",
    "\n",
    "    valid_X = trainX[:10000, :, :]\n",
    "    valid_Y = trainy[:10000]\n",
    "\n",
    "    return classes, (train_X, train_Y), (valid_X, valid_Y), (test_X, test_Y)\n",
    "\n",
    "def reshape_dataset(*data):\n",
    "    return [x.reshape(x.shape[0], x.shape[1] * x.shape[2]) for x in data]\n",
    "\n",
    "classes, (train_X, train_Y), (valid_X, valid_Y), (test_X, test_Y) = load_fashion_mnist()\n",
    "\n",
    "train_X, valid_X, test_X = reshape_dataset(train_X, valid_X, test_X)\n",
    "\n",
    "print(f'Train: X={train_X.shape}, y={train_Y.shape}')\n",
    "print(f'Valid: X={valid_X.shape}, y={valid_Y.shape}')\n",
    "print(f'Test: X={test_X.shape}, y={test_Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fec803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "num_classes = 10\n",
      "255\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_indices = np.unique(train_Y)\n",
    "print(class_indices)\n",
    "print(f'num_classes = {class_indices.shape[0]}')\n",
    "\n",
    "max_value = np.max(train_X)\n",
    "min_value = np.min(train_X)\n",
    "print(max_value)\n",
    "print(min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd2fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đổi giá trị của các hyperparameter bên dưới và\n",
    "# quan sát sự thay đổi của loss và quá trình training\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 0.01\n",
    "REG= 1e-5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553171d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    return X / 255.\n",
    "\n",
    "\n",
    "def reshape_x(X):\n",
    "    '''reshape X with N matrix (28 x 28) to X with N vector (784,)\n",
    "    Args:\n",
    "        X: np.ndarray, shape NxHxW (with H = W = 28)\n",
    "    Output:\n",
    "        X: np.ndarray, shape NxD (with D = H * W)\n",
    "    '''\n",
    "    return X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "\n",
    "\n",
    "def add_one(X):\n",
    "    '''Pad 1 as the 785th feature of train_X and test_X and valid_X\n",
    "    Args:\n",
    "        X: np.ndarray, shape N, D (with D=784)\n",
    "    Output:\n",
    "        X: np.ndarray, shape N, D + 1\n",
    "    '''\n",
    "    X = np.concatenate((X, np.ones(shape=(X.shape[0], 1))), axis=1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def create_one_hot(y, num_classes):\n",
    "    '''Example: y = [0, 2, 1, 0] with shape (4,), num_classes = 3\n",
    "        --> y_onehot = [[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]] shape: (4, 3)\n",
    "        Args:\n",
    "            y: np.ndarray, shape N\n",
    "            num_classes: int, (usually =np.unique(y).shape[0])\n",
    "        Outputs:\n",
    "            y_onehot: np.ndarray, shape Nxnum_classes\n",
    "    '''\n",
    "    y_onehot = np.zeros(shape=(y.shape[0], num_classes), dtype=np.int32)\n",
    "    y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "    \n",
    "    return y_onehot\n",
    "\n",
    "# def confusion_matrix(y_true, y_pred, num_classes):\n",
    "#     '''\n",
    "#     Args:\n",
    "#         y_true:\n",
    "#         y_pred:\n",
    "#     Outputs:\n",
    "#         m: \n",
    "#     '''\n",
    "#     pass\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efbadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "from trainer import batch_train, minibatch_train\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mnist_classification(use_batch_train=True):\n",
    "    # Load data from file\n",
    "    # Make sure that fashion-mnist/*.gz is in data/\n",
    "    classes, (train_X, train_Y), (valid_X, valid_Y), (test_X, test_Y) = load_fashion_mnist()\n",
    "    train_X, valid_X, test_X = normalize(train_X), normalize(valid_X), normalize(test_X)\n",
    "    train_X, valid_X, test_X = reshape_x(train_X), reshape_x(valid_X), reshape_x(test_X)\n",
    "    \n",
    "    num_classes = np.unique(train_Y).shape[0]\n",
    "\n",
    "    train_X = add_one(train_X)\n",
    "    valid_X = add_one(valid_X)\n",
    "    test_X = add_one(test_X)\n",
    "\n",
    "    train_Y = create_one_hot(train_Y, num_classes)\n",
    "    valid_Y = create_one_hot(valid_Y, num_classes)\n",
    "\n",
    "    # Create NN classifier\n",
    "    model = NeuralNetwork(learning_rate=LEARNING_RATE, num_classes = 10, reg=REG)\n",
    "    model.add_layers(128, 'relu')\n",
    "    model.add_layers(256, 'relu')\n",
    "    model.add_layers(100, 'relu')\n",
    "    model.add_layers(64, 'relu')\n",
    "    model.add_layers(num_classes, 'softmax')\n",
    "\n",
    "    if use_batch_train:\n",
    "        #Batch training - train all dataset\n",
    "        batch_train(train_X, train_Y, EPOCHS, model)\n",
    "    else:\n",
    "        #Minibatch training - training dataset using Minibatch approach\n",
    "        minibatch_train(train_X, train_Y, EPOCHS, BATCH_SIZE, num_classes, model)\n",
    "\n",
    "    metrics = confusion_matrix(test_Y, model.predict(test_X))\n",
    "    print(\"Confusion metrix: \")\n",
    "    print(metrics)\n",
    "\n",
    "    print(\"Accuracy: \")\n",
    "    print(metrics.trace() / test_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac9cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0: 0.0077\n",
      "Loss epoch 1: 0.0077\n",
      "Loss epoch 2: 0.0077\n",
      "Loss epoch 3: 0.0077\n",
      "Loss epoch 4: 0.0077\n",
      "Loss epoch 5: 0.0077\n",
      "Loss epoch 6: 0.0077\n",
      "Loss epoch 7: 0.0077\n",
      "Loss epoch 8: 0.0076\n",
      "Loss epoch 9: 0.0076\n",
      "Loss epoch 10: 0.0076\n",
      "Loss epoch 11: 0.0076\n",
      "Loss epoch 12: 0.0076\n",
      "Loss epoch 13: 0.0076\n",
      "Loss epoch 14: 0.0076\n",
      "Loss epoch 15: 0.0076\n",
      "Loss epoch 16: 0.0076\n",
      "Loss epoch 17: 0.0076\n",
      "Loss epoch 18: 0.0076\n",
      "Loss epoch 19: 0.0075\n",
      "Loss epoch 20: 0.0075\n",
      "Loss epoch 21: 0.0075\n",
      "Loss epoch 22: 0.0075\n",
      "Loss epoch 23: 0.0075\n",
      "Loss epoch 24: 0.0075\n",
      "Loss epoch 25: 0.0075\n",
      "Loss epoch 26: 0.0075\n",
      "Loss epoch 27: 0.0075\n",
      "Loss epoch 28: 0.0075\n",
      "Loss epoch 29: 0.0075\n",
      "Loss epoch 30: 0.0075\n",
      "Loss epoch 31: 0.0074\n",
      "Loss epoch 32: 0.0074\n",
      "Loss epoch 33: 0.0074\n",
      "Loss epoch 34: 0.0074\n",
      "Loss epoch 35: 0.0074\n",
      "Loss epoch 36: 0.0074\n",
      "Loss epoch 37: 0.0074\n",
      "Loss epoch 38: 0.0074\n",
      "Loss epoch 39: 0.0074\n",
      "Loss epoch 40: 0.0074\n",
      "Loss epoch 41: 0.0074\n",
      "Loss epoch 42: 0.0073\n",
      "Loss epoch 43: 0.0073\n",
      "Loss epoch 44: 0.0073\n",
      "Loss epoch 45: 0.0073\n",
      "Loss epoch 46: 0.0073\n",
      "Loss epoch 47: 0.0073\n",
      "Loss epoch 48: 0.0073\n",
      "Loss epoch 49: 0.0073\n",
      "Loss epoch 50: 0.0073\n",
      "Loss epoch 51: 0.0073\n",
      "Loss epoch 52: 0.0073\n",
      "Loss epoch 53: 0.0073\n",
      "Loss epoch 54: 0.0072\n",
      "Loss epoch 55: 0.0072\n",
      "Loss epoch 56: 0.0072\n",
      "Loss epoch 57: 0.0072\n",
      "Loss epoch 58: 0.0072\n",
      "Loss epoch 59: 0.0072\n",
      "Loss epoch 60: 0.0072\n",
      "Loss epoch 61: 0.0072\n",
      "Loss epoch 62: 0.0072\n",
      "Loss epoch 63: 0.0072\n",
      "Loss epoch 64: 0.0072\n",
      "Loss epoch 65: 0.0071\n",
      "Loss epoch 66: 0.0071\n",
      "Loss epoch 67: 0.0071\n",
      "Loss epoch 68: 0.0071\n",
      "Loss epoch 69: 0.0071\n",
      "Loss epoch 70: 0.0071\n",
      "Loss epoch 71: 0.0071\n",
      "Loss epoch 72: 0.0071\n",
      "Loss epoch 73: 0.0071\n",
      "Loss epoch 74: 0.0071\n",
      "Loss epoch 75: 0.0070\n",
      "Loss epoch 76: 0.0070\n",
      "Loss epoch 77: 0.0070\n",
      "Loss epoch 78: 0.0070\n",
      "Loss epoch 79: 0.0070\n",
      "Loss epoch 80: 0.0070\n",
      "Loss epoch 81: 0.0070\n",
      "Loss epoch 82: 0.0070\n",
      "Loss epoch 83: 0.0070\n",
      "Loss epoch 84: 0.0070\n",
      "Loss epoch 85: 0.0069\n",
      "Loss epoch 86: 0.0069\n",
      "Loss epoch 87: 0.0069\n",
      "Loss epoch 88: 0.0069\n",
      "Loss epoch 89: 0.0069\n",
      "Loss epoch 90: 0.0069\n",
      "Loss epoch 91: 0.0069\n",
      "Loss epoch 92: 0.0069\n",
      "Loss epoch 93: 0.0069\n",
      "Loss epoch 94: 0.0068\n",
      "Loss epoch 95: 0.0068\n",
      "Loss epoch 96: 0.0068\n",
      "Loss epoch 97: 0.0068\n",
      "Loss epoch 98: 0.0068\n",
      "Loss epoch 99: 0.0068\n",
      "Loss epoch 100: 0.0068\n",
      "Loss epoch 101: 0.0068\n",
      "Loss epoch 102: 0.0067\n",
      "Loss epoch 103: 0.0067\n",
      "Loss epoch 104: 0.0067\n",
      "Loss epoch 105: 0.0067\n",
      "Loss epoch 106: 0.0067\n",
      "Loss epoch 107: 0.0067\n",
      "Loss epoch 108: 0.0067\n",
      "Loss epoch 109: 0.0067\n",
      "Loss epoch 110: 0.0066\n",
      "Loss epoch 111: 0.0066\n",
      "Loss epoch 112: 0.0066\n",
      "Loss epoch 113: 0.0066\n",
      "Loss epoch 114: 0.0066\n",
      "Loss epoch 115: 0.0066\n",
      "Loss epoch 116: 0.0066\n",
      "Loss epoch 117: 0.0066\n",
      "Loss epoch 118: 0.0065\n",
      "Loss epoch 119: 0.0065\n",
      "Loss epoch 120: 0.0065\n",
      "Loss epoch 121: 0.0065\n",
      "Loss epoch 122: 0.0065\n",
      "Loss epoch 123: 0.0065\n",
      "Loss epoch 124: 0.0064\n",
      "Loss epoch 125: 0.0064\n",
      "Loss epoch 126: 0.0064\n",
      "Loss epoch 127: 0.0064\n",
      "Loss epoch 128: 0.0064\n",
      "Loss epoch 129: 0.0064\n",
      "Loss epoch 130: 0.0064\n",
      "Loss epoch 131: 0.0063\n",
      "Loss epoch 132: 0.0063\n",
      "Loss epoch 133: 0.0063\n",
      "Loss epoch 134: 0.0063\n",
      "Loss epoch 135: 0.0063\n",
      "Loss epoch 136: 0.0063\n",
      "Loss epoch 137: 0.0062\n",
      "Loss epoch 138: 0.0062\n",
      "Loss epoch 139: 0.0062\n",
      "Loss epoch 140: 0.0062\n",
      "Loss epoch 141: 0.0062\n",
      "Loss epoch 142: 0.0062\n",
      "Loss epoch 143: 0.0061\n",
      "Loss epoch 144: 0.0061\n",
      "Loss epoch 145: 0.0061\n",
      "Loss epoch 146: 0.0061\n",
      "Loss epoch 147: 0.0061\n",
      "Loss epoch 148: 0.0060\n",
      "Loss epoch 149: 0.0060\n",
      "Loss epoch 150: 0.0060\n",
      "Loss epoch 151: 0.0060\n",
      "Loss epoch 152: 0.0060\n",
      "Loss epoch 153: 0.0060\n",
      "Loss epoch 154: 0.0059\n",
      "Loss epoch 155: 0.0059\n",
      "Loss epoch 156: 0.0059\n",
      "Loss epoch 157: 0.0059\n",
      "Loss epoch 158: 0.0059\n",
      "Loss epoch 159: 0.0058\n",
      "Loss epoch 160: 0.0058\n",
      "Loss epoch 161: 0.0058\n",
      "Loss epoch 162: 0.0058\n",
      "Loss epoch 163: 0.0058\n",
      "Loss epoch 164: 0.0058\n",
      "Loss epoch 165: 0.0057\n",
      "Loss epoch 166: 0.0057\n",
      "Loss epoch 167: 0.0057\n",
      "Loss epoch 168: 0.0057\n",
      "Loss epoch 169: 0.0057\n",
      "Loss epoch 170: 0.0056\n",
      "Loss epoch 171: 0.0056\n",
      "Loss epoch 172: 0.0056\n",
      "Loss epoch 173: 0.0056\n",
      "Loss epoch 174: 0.0056\n",
      "Loss epoch 175: 0.0055\n",
      "Loss epoch 176: 0.0055\n",
      "Loss epoch 177: 0.0055\n",
      "Loss epoch 178: 0.0055\n",
      "Loss epoch 179: 0.0055\n",
      "Loss epoch 180: 0.0055\n",
      "Loss epoch 181: 0.0054\n",
      "Loss epoch 182: 0.0054\n",
      "Loss epoch 183: 0.0054\n",
      "Loss epoch 184: 0.0054\n",
      "Loss epoch 185: 0.0054\n",
      "Loss epoch 186: 0.0053\n",
      "Loss epoch 187: 0.0053\n",
      "Loss epoch 188: 0.0053\n",
      "Loss epoch 189: 0.0053\n",
      "Loss epoch 190: 0.0053\n",
      "Loss epoch 191: 0.0052\n",
      "Loss epoch 192: 0.0052\n",
      "Loss epoch 193: 0.0052\n",
      "Loss epoch 194: 0.0052\n",
      "Loss epoch 195: 0.0052\n",
      "Loss epoch 196: 0.0052\n",
      "Loss epoch 197: 0.0051\n",
      "Loss epoch 198: 0.0051\n",
      "Loss epoch 199: 0.0051\n",
      "Loss epoch 200: 0.0051\n",
      "Loss epoch 201: 0.0051\n",
      "Loss epoch 202: 0.0050\n",
      "Loss epoch 203: 0.0050\n",
      "Loss epoch 204: 0.0050\n",
      "Loss epoch 205: 0.0050\n",
      "Loss epoch 206: 0.0050\n",
      "Loss epoch 207: 0.0050\n",
      "Loss epoch 208: 0.0049\n",
      "Loss epoch 209: 0.0049\n",
      "Loss epoch 210: 0.0049\n",
      "Loss epoch 211: 0.0049\n",
      "Loss epoch 212: 0.0049\n",
      "Loss epoch 213: 0.0049\n",
      "Loss epoch 214: 0.0048\n",
      "Loss epoch 215: 0.0048\n",
      "Loss epoch 216: 0.0048\n",
      "Loss epoch 217: 0.0048\n",
      "Loss epoch 218: 0.0048\n",
      "Loss epoch 219: 0.0048\n",
      "Loss epoch 220: 0.0047\n",
      "Loss epoch 221: 0.0047\n",
      "Loss epoch 222: 0.0047\n",
      "Loss epoch 223: 0.0047\n",
      "Loss epoch 224: 0.0047\n",
      "Loss epoch 225: 0.0047\n",
      "Loss epoch 226: 0.0046\n",
      "Loss epoch 227: 0.0046\n",
      "Loss epoch 228: 0.0046\n",
      "Loss epoch 229: 0.0046\n",
      "Loss epoch 230: 0.0046\n",
      "Loss epoch 231: 0.0046\n",
      "Loss epoch 232: 0.0045\n",
      "Loss epoch 233: 0.0045\n",
      "Loss epoch 234: 0.0045\n",
      "Loss epoch 235: 0.0045\n",
      "Loss epoch 236: 0.0045\n",
      "Loss epoch 237: 0.0045\n",
      "Loss epoch 238: 0.0044\n",
      "Loss epoch 239: 0.0044\n",
      "Loss epoch 240: 0.0044\n",
      "Loss epoch 241: 0.0044\n",
      "Loss epoch 242: 0.0044\n",
      "Loss epoch 243: 0.0044\n",
      "Loss epoch 244: 0.0044\n",
      "Loss epoch 245: 0.0043\n",
      "Loss epoch 246: 0.0043\n",
      "Loss epoch 247: 0.0043\n",
      "Loss epoch 248: 0.0043\n",
      "Loss epoch 249: 0.0043\n",
      "Loss epoch 250: 0.0043\n",
      "Loss epoch 251: 0.0042\n",
      "Loss epoch 252: 0.0042\n",
      "Loss epoch 253: 0.0042\n",
      "Loss epoch 254: 0.0042\n",
      "Loss epoch 255: 0.0042\n",
      "Loss epoch 256: 0.0042\n",
      "Loss epoch 257: 0.0042\n",
      "Loss epoch 258: 0.0041\n",
      "Loss epoch 259: 0.0041\n",
      "Loss epoch 260: 0.0041\n",
      "Loss epoch 261: 0.0041\n",
      "Loss epoch 262: 0.0041\n",
      "Loss epoch 263: 0.0041\n",
      "Loss epoch 264: 0.0041\n",
      "Loss epoch 265: 0.0041\n",
      "Loss epoch 266: 0.0040\n",
      "Loss epoch 267: 0.0040\n",
      "Loss epoch 268: 0.0040\n",
      "Loss epoch 269: 0.0040\n",
      "Loss epoch 270: 0.0040\n",
      "Loss epoch 271: 0.0040\n",
      "Loss epoch 272: 0.0040\n",
      "Loss epoch 273: 0.0040\n",
      "Loss epoch 274: 0.0039\n",
      "Loss epoch 275: 0.0039\n",
      "Loss epoch 276: 0.0039\n",
      "Loss epoch 277: 0.0039\n",
      "Loss epoch 278: 0.0039\n",
      "Loss epoch 279: 0.0039\n",
      "Loss epoch 280: 0.0039\n",
      "Loss epoch 281: 0.0039\n",
      "Loss epoch 282: 0.0038\n",
      "Loss epoch 283: 0.0038\n",
      "Loss epoch 284: 0.0038\n",
      "Loss epoch 285: 0.0038\n",
      "Loss epoch 286: 0.0038\n",
      "Loss epoch 287: 0.0038\n",
      "Loss epoch 288: 0.0038\n",
      "Loss epoch 289: 0.0038\n",
      "Loss epoch 290: 0.0038\n",
      "Loss epoch 291: 0.0037\n",
      "Loss epoch 292: 0.0037\n",
      "Loss epoch 293: 0.0037\n",
      "Loss epoch 294: 0.0037\n",
      "Loss epoch 295: 0.0037\n",
      "Loss epoch 296: 0.0037\n",
      "Loss epoch 297: 0.0037\n",
      "Loss epoch 298: 0.0037\n",
      "Loss epoch 299: 0.0037\n",
      "Confusion metrix: \n",
      "[[796   5  27 102  15   0  38   1  15   1]\n",
      " [ 18 904   7  51  18   0   0   0   2   0]\n",
      " [ 27   1 490  12 389   0  39   0  42   0]\n",
      " [ 67  17   3 830  45   0  35   0   3   0]\n",
      " [  9   5 158  87 713   0  11   0  16   1]\n",
      " [  2   0   0   4   3  76   5 476  99 335]\n",
      " [273   3 171  59 339   0  91   0  64   0]\n",
      " [  0   0   0   0   0   0   0 884   9 107]\n",
      " [  3   2  15  16   2   0  42  23 896   1]\n",
      " [  0   0   0   2   7   0   0  56   2 933]]\n",
      "Accuracy: \n",
      "0.6613\n"
     ]
    }
   ],
   "source": [
    "mnist_classification(use_batch_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9f2b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f086d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plot_loss(losses=[1, 3, 4], title='loss function', color='ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391b0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0: 0.8648\n",
      "Loss epoch 1: 0.3170\n",
      "Loss epoch 2: 0.3062\n",
      "Loss epoch 3: 0.2085\n",
      "Loss epoch 4: 0.1717\n",
      "Loss epoch 5: 0.1973\n",
      "Loss epoch 6: 0.1849\n",
      "Loss epoch 7: 0.1549\n",
      "Loss epoch 8: 0.1299\n",
      "Loss epoch 9: 0.1133\n",
      "Loss epoch 10: 0.1276\n",
      "Loss epoch 11: 0.1155\n",
      "Loss epoch 12: 0.1256\n",
      "Loss epoch 13: 0.1159\n",
      "Loss epoch 14: 0.0929\n",
      "Loss epoch 15: 0.0872\n",
      "Loss epoch 16: 0.0998\n",
      "Loss epoch 17: 0.1077\n",
      "Loss epoch 18: 0.0904\n",
      "Loss epoch 19: 0.0801\n",
      "Loss epoch 20: 0.0847\n",
      "Loss epoch 21: 0.0856\n",
      "Loss epoch 22: 0.0932\n",
      "Loss epoch 23: 0.0553\n",
      "Loss epoch 24: 0.0697\n",
      "Loss epoch 25: 0.0712\n",
      "Loss epoch 26: 0.0792\n",
      "Loss epoch 27: 0.0765\n",
      "Loss epoch 28: 0.0846\n",
      "Loss epoch 29: 0.0694\n",
      "Loss epoch 30: 0.0736\n",
      "Loss epoch 31: 0.0741\n",
      "Loss epoch 32: 0.0745\n",
      "Loss epoch 33: 0.0806\n",
      "Loss epoch 34: 0.0501\n",
      "Loss epoch 35: 0.0682\n",
      "Loss epoch 36: 0.0655\n",
      "Loss epoch 37: 0.0644\n",
      "Loss epoch 38: 0.0572\n",
      "Loss epoch 39: 0.0612\n",
      "Loss epoch 40: 0.0795\n",
      "Loss epoch 41: 0.0500\n",
      "Loss epoch 42: 0.0665\n",
      "Loss epoch 43: 0.0573\n",
      "Loss epoch 44: 0.0585\n",
      "Loss epoch 45: 0.0667\n",
      "Loss epoch 46: 0.0459\n",
      "Loss epoch 47: 0.0471\n",
      "Loss epoch 48: 0.0533\n",
      "Loss epoch 49: 0.0508\n",
      "Loss epoch 50: 0.0541\n",
      "Loss epoch 51: 0.0396\n",
      "Loss epoch 52: 0.0501\n",
      "Loss epoch 53: 0.0403\n",
      "Loss epoch 54: 0.0591\n",
      "Loss epoch 55: 0.0650\n",
      "Loss epoch 56: 0.0498\n",
      "Loss epoch 57: 0.0534\n",
      "Loss epoch 58: 0.0557\n",
      "Loss epoch 59: 0.0507\n",
      "Loss epoch 60: 0.0494\n",
      "Loss epoch 61: 0.0567\n",
      "Loss epoch 62: 0.0420\n",
      "Loss epoch 63: 0.0473\n",
      "Loss epoch 64: 0.0355\n",
      "Loss epoch 65: 0.0484\n",
      "Loss epoch 66: 0.0536\n",
      "Loss epoch 67: 0.0372\n",
      "Loss epoch 68: 0.0408\n",
      "Loss epoch 69: 0.0422\n",
      "Loss epoch 70: 0.0452\n",
      "Loss epoch 71: 0.0373\n",
      "Loss epoch 72: 0.0379\n",
      "Loss epoch 73: 0.0395\n",
      "Loss epoch 74: 0.0534\n",
      "Loss epoch 75: 0.0426\n",
      "Loss epoch 76: 0.0426\n",
      "Loss epoch 77: 0.0365\n",
      "Loss epoch 78: 0.0411\n",
      "Loss epoch 79: 0.0381\n",
      "Loss epoch 80: 0.0457\n",
      "Loss epoch 81: 0.0418\n",
      "Loss epoch 82: 0.0383\n",
      "Loss epoch 83: 0.0344\n",
      "Loss epoch 84: 0.0527\n",
      "Loss epoch 85: 0.0502\n",
      "Loss epoch 86: 0.0436\n",
      "Loss epoch 87: 0.0429\n",
      "Loss epoch 88: 0.0522\n",
      "Loss epoch 89: 0.0362\n",
      "Loss epoch 90: 0.0492\n",
      "Loss epoch 91: 0.0390\n",
      "Loss epoch 92: 0.0589\n",
      "Loss epoch 93: 0.0419\n",
      "Loss epoch 94: 0.0441\n",
      "Loss epoch 95: 0.0442\n",
      "Loss epoch 96: 0.0459\n",
      "Loss epoch 97: 0.0332\n",
      "Loss epoch 98: 0.0294\n",
      "Loss epoch 99: 0.0307\n",
      "Loss epoch 100: 0.0381\n",
      "Loss epoch 101: 0.0469\n",
      "Loss epoch 102: 0.0370\n",
      "Loss epoch 103: 0.0361\n",
      "Loss epoch 104: 0.0364\n",
      "Loss epoch 105: 0.0380\n",
      "Loss epoch 106: 0.0332\n",
      "Loss epoch 107: 0.0351\n",
      "Loss epoch 108: 0.0404\n",
      "Loss epoch 109: 0.0330\n",
      "Loss epoch 110: 0.0393\n",
      "Loss epoch 111: 0.0372\n",
      "Loss epoch 112: 0.0274\n",
      "Loss epoch 113: 0.0341\n",
      "Loss epoch 114: 0.0364\n",
      "Loss epoch 115: 0.0352\n",
      "Loss epoch 116: 0.0363\n",
      "Loss epoch 117: 0.0323\n",
      "Loss epoch 118: 0.0358\n",
      "Loss epoch 119: 0.0437\n",
      "Loss epoch 120: 0.0306\n",
      "Loss epoch 121: 0.0451\n",
      "Loss epoch 122: 0.0272\n",
      "Loss epoch 123: 0.0385\n",
      "Loss epoch 124: 0.0354\n",
      "Loss epoch 125: 0.0546\n",
      "Loss epoch 126: 0.0271\n",
      "Loss epoch 127: 0.0237\n",
      "Loss epoch 128: 0.0362\n",
      "Loss epoch 129: 0.0304\n",
      "Loss epoch 130: 0.0291\n",
      "Loss epoch 131: 0.0280\n",
      "Loss epoch 132: 0.0343\n",
      "Loss epoch 133: 0.0331\n",
      "Loss epoch 134: 0.0273\n",
      "Loss epoch 135: 0.0329\n",
      "Loss epoch 136: 0.0496\n",
      "Loss epoch 137: 0.0247\n",
      "Loss epoch 138: 0.0201\n",
      "Loss epoch 139: 0.0411\n",
      "Loss epoch 140: 0.0283\n",
      "Loss epoch 141: 0.0407\n",
      "Loss epoch 142: 0.0322\n",
      "Loss epoch 143: 0.0364\n",
      "Loss epoch 144: 0.0398\n",
      "Loss epoch 145: 0.0349\n",
      "Loss epoch 146: 0.0282\n",
      "Loss epoch 147: 0.0420\n",
      "Loss epoch 148: 0.0358\n",
      "Loss epoch 149: 0.0399\n",
      "Loss epoch 150: 0.0258\n",
      "Loss epoch 151: 0.0247\n",
      "Loss epoch 152: 0.0326\n",
      "Loss epoch 153: 0.0267\n",
      "Loss epoch 154: 0.0287\n",
      "Loss epoch 155: 0.0295\n",
      "Loss epoch 156: 0.0363\n",
      "Loss epoch 157: 0.0374\n",
      "Loss epoch 158: 0.0315\n",
      "Loss epoch 159: 0.0278\n",
      "Loss epoch 160: 0.0311\n",
      "Loss epoch 161: 0.0446\n",
      "Loss epoch 162: 0.0255\n",
      "Loss epoch 163: 0.0275\n",
      "Loss epoch 164: 0.0297\n",
      "Loss epoch 165: 0.0322\n",
      "Loss epoch 166: 0.0280\n",
      "Loss epoch 167: 0.0337\n",
      "Loss epoch 168: 0.0334\n",
      "Loss epoch 169: 0.0409\n",
      "Loss epoch 170: 0.0272\n",
      "Loss epoch 171: 0.0359\n",
      "Loss epoch 172: 0.0324\n",
      "Loss epoch 173: 0.0343\n",
      "Loss epoch 174: 0.0283\n",
      "Loss epoch 175: 0.0283\n",
      "Loss epoch 176: 0.0252\n",
      "Loss epoch 177: 0.0220\n",
      "Loss epoch 178: 0.0231\n",
      "Loss epoch 179: 0.0329\n",
      "Loss epoch 180: 0.0267\n",
      "Loss epoch 181: 0.0258\n",
      "Loss epoch 182: 0.0288\n",
      "Loss epoch 183: 0.0303\n",
      "Loss epoch 184: 0.0268\n",
      "Loss epoch 185: 0.0215\n",
      "Loss epoch 186: 0.0332\n",
      "Loss epoch 187: 0.0356\n",
      "Loss epoch 188: 0.0231\n",
      "Loss epoch 189: 0.0302\n",
      "Loss epoch 190: 0.0252\n",
      "Loss epoch 191: 0.0297\n",
      "Loss epoch 192: 0.0265\n",
      "Loss epoch 193: 0.0340\n",
      "Loss epoch 194: 0.0215\n",
      "Loss epoch 195: 0.0284\n",
      "Loss epoch 196: 0.0285\n",
      "Loss epoch 197: 0.0320\n",
      "Loss epoch 198: 0.0296\n",
      "Loss epoch 199: 0.0251\n",
      "Loss epoch 200: 0.0186\n",
      "Loss epoch 201: 0.0292\n",
      "Loss epoch 202: 0.0244\n",
      "Loss epoch 203: 0.0159\n",
      "Loss epoch 204: 0.0228\n",
      "Loss epoch 205: 0.0187\n",
      "Loss epoch 206: 0.0264\n",
      "Loss epoch 207: 0.0237\n",
      "Loss epoch 208: 0.0244\n",
      "Loss epoch 209: 0.0229\n",
      "Loss epoch 210: 0.0275\n",
      "Loss epoch 211: 0.0308\n",
      "Loss epoch 212: 0.0212\n",
      "Loss epoch 213: 0.0245\n",
      "Loss epoch 214: 0.0239\n",
      "Loss epoch 215: 0.0313\n",
      "Loss epoch 216: 0.0339\n",
      "Loss epoch 217: 0.0198\n",
      "Loss epoch 218: 0.0256\n",
      "Loss epoch 219: 0.0235\n",
      "Loss epoch 220: 0.0294\n",
      "Loss epoch 221: 0.0321\n",
      "Loss epoch 222: 0.0226\n",
      "Loss epoch 223: 0.0227\n",
      "Loss epoch 224: 0.0283\n",
      "Loss epoch 225: 0.0255\n",
      "Loss epoch 226: 0.0282\n",
      "Loss epoch 227: 0.0216\n",
      "Loss epoch 228: 0.0261\n",
      "Loss epoch 229: 0.0225\n",
      "Loss epoch 230: 0.0294\n",
      "Loss epoch 231: 0.0262\n",
      "Loss epoch 232: 0.0316\n",
      "Loss epoch 233: 0.0184\n",
      "Loss epoch 234: 0.0199\n",
      "Loss epoch 235: 0.0302\n",
      "Loss epoch 236: 0.0227\n",
      "Loss epoch 237: 0.0225\n",
      "Loss epoch 238: 0.0270\n",
      "Loss epoch 239: 0.0277\n",
      "Loss epoch 240: 0.0276\n",
      "Loss epoch 241: 0.0237\n",
      "Loss epoch 242: 0.0229\n",
      "Loss epoch 243: 0.0182\n",
      "Loss epoch 244: 0.0280\n",
      "Loss epoch 245: 0.0195\n",
      "Loss epoch 246: 0.0243\n",
      "Loss epoch 247: 0.0243\n",
      "Loss epoch 248: 0.0248\n",
      "Loss epoch 249: 0.0216\n",
      "Loss epoch 250: 0.0286\n",
      "Loss epoch 251: 0.0216\n",
      "Loss epoch 252: 0.0199\n",
      "Loss epoch 253: 0.0214\n",
      "Loss epoch 254: 0.0236\n",
      "Loss epoch 255: 0.0226\n",
      "Loss epoch 256: 0.0267\n",
      "Loss epoch 257: 0.0227\n",
      "Loss epoch 258: 0.0230\n",
      "Loss epoch 259: 0.0252\n",
      "Loss epoch 260: 0.0239\n",
      "Loss epoch 261: 0.0192\n",
      "Loss epoch 262: 0.0259\n",
      "Loss epoch 263: 0.0318\n",
      "Loss epoch 264: 0.0241\n",
      "Loss epoch 265: 0.0206\n",
      "Loss epoch 266: 0.0217\n",
      "Loss epoch 267: 0.0276\n",
      "Loss epoch 268: 0.0243\n",
      "Loss epoch 269: 0.0259\n",
      "Loss epoch 270: 0.0264\n",
      "Loss epoch 271: 0.0209\n",
      "Loss epoch 272: 0.0191\n",
      "Loss epoch 273: 0.0226\n",
      "Loss epoch 274: 0.0254\n",
      "Loss epoch 275: 0.0206\n",
      "Loss epoch 276: 0.0238\n",
      "Loss epoch 277: 0.0221\n",
      "Loss epoch 278: 0.0225\n",
      "Loss epoch 279: 0.0278\n",
      "Loss epoch 280: 0.0234\n",
      "Loss epoch 281: 0.0222\n",
      "Loss epoch 282: 0.0229\n",
      "Loss epoch 283: 0.0230\n",
      "Loss epoch 284: 0.0187\n",
      "Loss epoch 285: 0.0234\n",
      "Loss epoch 286: 0.0252\n",
      "Loss epoch 287: 0.0196\n",
      "Loss epoch 288: 0.0198\n",
      "Loss epoch 289: 0.0209\n",
      "Loss epoch 290: 0.0167\n",
      "Loss epoch 291: 0.0176\n",
      "Loss epoch 292: 0.0264\n",
      "Loss epoch 293: 0.0210\n",
      "Loss epoch 294: 0.0202\n",
      "Loss epoch 295: 0.0228\n",
      "Loss epoch 296: 0.0190\n",
      "Loss epoch 297: 0.0155\n",
      "Loss epoch 298: 0.0234\n",
      "Loss epoch 299: 0.0162\n",
      "Confusion metrix: \n",
      "[[843   2   8  40   3   0  95   0   9   0]\n",
      " [  9 969   2  15   3   0   1   0   1   0]\n",
      " [ 53   2 739  14 132   0  58   0   2   0]\n",
      " [ 31  14   9 874  58   0  11   0   3   0]\n",
      " [ 10   5  94  30 793   0  66   0   2   0]\n",
      " [  1   0   0   1   0 959   0  29   2   8]\n",
      " [198   2  96  47  79   0 558   0  20   0]\n",
      " [  0   0   0   0   0  26   0 963   0  11]\n",
      " [ 20   1   4  15   3   3  12   5 937   0]\n",
      " [  1   0   0   1   0  18   0  70   0 910]]\n",
      "Accuracy: \n",
      "0.8545\n"
     ]
    }
   ],
   "source": [
    "mnist_classification(use_batch_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cef4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
